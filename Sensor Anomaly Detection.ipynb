{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bearing Failure Anomaly Detection\n",
    "In this workbook, we use an autoencoder neural network to identify vibrational anomalies from sensor readings in a set of bearings. The goal is to be able to predict future bearing failures before they happen. The vibrational sensor readings are from the NASA Acoustics and Vibration Database. Each data set consists of individual files that are 1-second vibration signal snapshots recorded at 10 minute intervals. Each file contains 20,480 sensor data points that were obtained by reading the bearing sensors at a sampling rate of 20 kHz.\n",
    "\n",
    "This autoencoder neural network model is created using Long Short-Term Memory (LSTM) recurrent neural network (RNN) cells within the Keras / TensorFlow framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# from sklearn.externals import joblib # not worked string\n",
    "import joblib\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(2712) \n",
    "\n",
    "\n",
    "# tf.math.log.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "#seed(10)\n",
    "#set_random_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and pre-processing\n",
    "An assumption is that mechanical degradation in the bearings occurs gradually over time; therefore, we use one datapoint every 10 minutes in the analysis. Each 10 minute datapoint is aggregated by using the mean absolute value of the vibration recordings over the 20,480 datapoints in each file. We then merge together everything in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, average and merge sensor samples\n",
    "# data_dir = 'data/bearing_data'\n",
    "data_dir = '/Users/vistratov/dev_data/data/bearing_data'\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    dataset = pd.read_csv(os.path.join(data_dir, filename), sep='\\t')\n",
    "    dataset_mean_abs = np.array(dataset.abs().mean())\n",
    "    dataset_mean_abs = pd.DataFrame(dataset_mean_abs.reshape(1,4))\n",
    "    dataset_mean_abs.index = [filename]\n",
    "    merged_data = merged_data.append(dataset_mean_abs)\n",
    "    \n",
    "merged_data.columns = ['Bearing 1', 'Bearing 2', 'Bearing 3', 'Bearing 4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data file index to datetime and sort in chronological order\n",
    "merged_data.index = pd.to_datetime(merged_data.index, format='%Y.%m.%d.%H.%M.%S')\n",
    "merged_data = merged_data.sort_index()\n",
    "merged_data.to_csv('Averaged_BearingTest_Dataset.csv')\n",
    "print(\"Dataset shape:\", merged_data.shape)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train/test data\n",
    "Before setting up the models, we need to define train/test data. To do this, we perform a simple split where we train on the first part of the dataset (which should represent normal operating conditions) and test on the remaining parts of the dataset leading up to the bearing failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = merged_data['2004-02-12 10:52:39': '2004-02-15 12:52:39']\n",
    "test = merged_data['2004-02-15 12:52:39':]\n",
    "print(\"Training dataset shape:\", train.shape)\n",
    "print(\"Test dataset shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(train['Bearing 1'], label='Bearing 1', color='blue', animated = True, linewidth=1)\n",
    "ax.plot(train['Bearing 2'], label='Bearing 2', color='red', animated = True, linewidth=1)\n",
    "ax.plot(train['Bearing 3'], label='Bearing 3', color='green', animated = True, linewidth=1)\n",
    "ax.plot(train['Bearing 4'], label='Bearing 4', color='black', animated = True, linewidth=1)\n",
    "plt.legend(loc='lower left')\n",
    "ax.set_title('Bearing Sensor Training Data', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get a different perspective of the data by transforming the signal from the time domain to the frequency domain using a discrete Fourier transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data from the time domain to the frequency domain using fast Fourier transform\n",
    "train_fft = np.fft.fft(train)\n",
    "test_fft = np.fft.fft(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies of the healthy sensor signal\n",
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(train_fft[:,0].real, label='Bearing 1', color='blue', animated = True, linewidth=1)\n",
    "ax.plot(train_fft[:,1].imag, label='Bearing 2', color='red', animated = True, linewidth=1)\n",
    "ax.plot(train_fft[:,2].real, label='Bearing 3', color='green', animated = True, linewidth=1)\n",
    "ax.plot(train_fft[:,3].real, label='Bearing 4', color='black', animated = True, linewidth=1)\n",
    "plt.legend(loc='lower left')\n",
    "ax.set_title('Bearing Sensor Training Frequency Data', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies of the degrading sensor signal\n",
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(test_fft[:,0].real, label='Bearing 1', color='blue', animated = True, linewidth=1)\n",
    "ax.plot(test_fft[:,1].imag, label='Bearing 2', color='red', animated = True, linewidth=1)\n",
    "ax.plot(test_fft[:,2].real, label='Bearing 3', color='green', animated = True, linewidth=1)\n",
    "ax.plot(test_fft[:,3].real, label='Bearing 4', color='black', animated = True, linewidth=1)\n",
    "plt.legend(loc='lower left')\n",
    "ax.set_title('Bearing Sensor Test Frequency Data', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train)\n",
    "X_test = scaler.transform(test)\n",
    "scaler_filename = \"scaler_data\"\n",
    "joblib.dump(scaler, scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape inputs for LSTM [samples, timesteps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "print(\"Test data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the autoencoder network model\n",
    "def autoencoder_model(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    L1 = LSTM(16, activation='relu', return_sequences=True, \n",
    "              kernel_regularizer=regularizers.l2(0.00))(inputs)\n",
    "    L2 = LSTM(4, activation='relu', return_sequences=False)(L1)\n",
    "    L3 = RepeatVector(X.shape[1])(L2)\n",
    "    L4 = LSTM(4, activation='relu', return_sequences=True)(L3)\n",
    "    L5 = LSTM(16, activation='relu', return_sequences=True)(L4)\n",
    "    output = TimeDistributed(Dense(X.shape[2]))(L5)    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the autoencoder model\n",
    "model = autoencoder_model(X_train)\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the data\n",
    "nb_epochs = 100\n",
    "batch_size = 10\n",
    "history = model.fit(X_train, X_train, epochs=nb_epochs, batch_size=batch_size, validation_split=0.05).history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training losses\n",
    "fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "ax.plot(history['loss'], 'b', label='Train', linewidth=2)\n",
    "ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\n",
    "ax.set_title('Model loss', fontsize=16)\n",
    "ax.set_ylabel('Loss (mae)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Loss Function\n",
    "By plotting the distribution of the calculated loss in the training set, one can use this to identify a suitable threshold value for identifying an anomaly. In doing this, one can make sure that this threshold is set above the “noise level” and that any flagged anomalies should be statistically significant above the background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss distribution of the training set\n",
    "X_pred = model.predict(X_train)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = pd.DataFrame(X_pred, columns=train.columns)\n",
    "X_pred.index = train.index\n",
    "\n",
    "scored = pd.DataFrame(index=train.index)\n",
    "Xtrain = X_train.reshape(X_train.shape[0], X_train.shape[2])\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtrain), axis = 1)\n",
    "plt.figure(figsize=(16,9), dpi=80)\n",
    "plt.title('Loss Distribution', fontsize=16)\n",
    "sns.distplot(scored['Loss_mae'], bins = 20, kde= True, color = 'blue');\n",
    "plt.xlim([0.0,.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above loss distribution, let's try a threshold value of 0.275 for flagging an anomaly. We can then calculate the loss in the test set to check when the output crosses the anomaly threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the loss on the test set\n",
    "X_pred = model.predict(X_test)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = pd.DataFrame(X_pred, columns=test.columns)\n",
    "X_pred.index = test.index\n",
    "\n",
    "scored = pd.DataFrame(index=test.index)\n",
    "Xtest = X_test.reshape(X_test.shape[0], X_test.shape[2])\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-Xtest), axis = 1)\n",
    "scored['Threshold'] = 0.275\n",
    "scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "scored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the same metrics for the training set \n",
    "# and merge all data in a single dataframe for plotting\n",
    "X_pred_train = model.predict(X_train)\n",
    "X_pred_train = X_pred_train.reshape(X_pred_train.shape[0], X_pred_train.shape[2])\n",
    "X_pred_train = pd.DataFrame(X_pred_train, columns=train.columns)\n",
    "X_pred_train.index = train.index\n",
    "\n",
    "scored_train = pd.DataFrame(index=train.index)\n",
    "scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-Xtrain), axis = 1)\n",
    "scored_train['Threshold'] = 0.275\n",
    "scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\n",
    "scored = pd.concat([scored_train, scored])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having calculated the loss distribution and the anomaly threshold, we can visualize the model output in the time leading up to the bearing failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bearing failure time plot\n",
    "scored.plot(logy=True,  figsize=(16,9), ylim=[1e-2,1e2], color=['blue','red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis approach is able to flag the upcoming bearing malfunction well in advance of the actual physical failure. It is important to define a suitable threshold value for flagging anomalies while avoiding too many false positives during normal operating conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all model information, including weights, in h5 format\n",
    "model.save(\"Cloud_model.h5\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
